<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Haozhe Jiang</title><link>https://astro-eric.github.io/</link><description>Recent content on Haozhe Jiang</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 16 Nov 2025 23:29:21 +0530</lastBuildDate><atom:link href="https://astro-eric.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Beijing</title><link>https://astro-eric.github.io/gallery/beijing/</link><pubDate>Sun, 16 Nov 2025 23:29:21 +0530</pubDate><guid>https://astro-eric.github.io/gallery/beijing/</guid><description/></item><item><title>California</title><link>https://astro-eric.github.io/gallery/california/</link><pubDate>Sun, 16 Nov 2025 23:29:21 +0530</pubDate><guid>https://astro-eric.github.io/gallery/california/</guid><description/></item><item><title>Can Transformers Do Everything, and Undo It Too?</title><link>https://astro-eric.github.io/blogs/surjective/</link><pubDate>Sun, 16 Nov 2025 23:29:21 +0530</pubDate><guid>https://astro-eric.github.io/blogs/surjective/</guid><description>&lt;h1 id="large-language-models-are-surjective-injective-invertible"&gt;Large Language Models are Surjective? Injective? Invertible?&lt;/h1&gt;
&lt;p&gt;Recently, there have been discussions on functional properties of Transformers, the basic building block of Large Language Models (LLM) and many other generative models. &lt;a href="https://arxiv.org/abs/2508.19445"&gt;My paper&lt;/a&gt; ([1]) proves that Transformers can output anything given an appropriate input (surjective). After a few months, &lt;a href="https://arxiv.org/abs/2510.15511"&gt;a followup&lt;/a&gt; ([2]) proves that LLMs always send different inputs to different outputs (injective), and hence we can invert the outputs back to inputs. With these claims in mind, wild thoughts about generative models start flowing around. Are jailbreaks fundamentally unavoidable? Are Transformers lossless compression of knowledge? Do we have no privacy when we use LLMs? Does combining the two papers imply that LLMs are bijective? In this blog, we clarify the concepts of surjectivity, injectivity and invertibility that appear in these papers and explain their true implications.&lt;/p&gt;</description></item><item><title>Hangzhou</title><link>https://astro-eric.github.io/gallery/hangzhou/</link><pubDate>Sun, 16 Nov 2025 23:29:21 +0530</pubDate><guid>https://astro-eric.github.io/gallery/hangzhou/</guid><description/></item><item><title>Hawaii</title><link>https://astro-eric.github.io/gallery/hawaii/</link><pubDate>Sun, 16 Nov 2025 23:29:21 +0530</pubDate><guid>https://astro-eric.github.io/gallery/hawaii/</guid><description/></item><item><title>Princeton</title><link>https://astro-eric.github.io/gallery/princeton/</link><pubDate>Sun, 16 Nov 2025 23:29:21 +0530</pubDate><guid>https://astro-eric.github.io/gallery/princeton/</guid><description/></item><item><title>Rwanda</title><link>https://astro-eric.github.io/gallery/rwanda/</link><pubDate>Sun, 16 Nov 2025 23:29:21 +0530</pubDate><guid>https://astro-eric.github.io/gallery/rwanda/</guid><description/></item><item><title>Seattle</title><link>https://astro-eric.github.io/gallery/seattle/</link><pubDate>Sun, 16 Nov 2025 23:29:21 +0530</pubDate><guid>https://astro-eric.github.io/gallery/seattle/</guid><description/></item><item><title>Shanghai</title><link>https://astro-eric.github.io/gallery/shanghai/</link><pubDate>Sun, 16 Nov 2025 23:29:21 +0530</pubDate><guid>https://astro-eric.github.io/gallery/shanghai/</guid><description/></item><item><title>Vienna</title><link>https://astro-eric.github.io/gallery/vienna/</link><pubDate>Sun, 16 Nov 2025 23:29:21 +0530</pubDate><guid>https://astro-eric.github.io/gallery/vienna/</guid><description/></item><item><title>Yili</title><link>https://astro-eric.github.io/gallery/yili/</link><pubDate>Sun, 16 Nov 2025 23:29:21 +0530</pubDate><guid>https://astro-eric.github.io/gallery/yili/</guid><description/></item><item><title>Publications</title><link>https://astro-eric.github.io/publications/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://astro-eric.github.io/publications/</guid><description>&lt;div class="pub-search-index" style="display:none"&gt; On Surjectivity of Neural Networks: Can you elicit any behavior from your model? Haozhe Jiang, Nika Haghtalab Theory Deep Learning We prove that a lot of practical architures for generative models are surjective, regardless of how they are trained. This means that any output, including undesirable ones, can be elicited by some input, leading to a fundamental vulnerability to jailbreaks.
 

 A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning Haozhe Jiang, Qiwen Cui, Zhihan Xiong, Maryam Fazel, Simon S Du Theory Game Theory ICLR 2024 Can we track Equilibria in non-stationary Multi-agent systems? We comprehensively analyze this problem and find most non-stationary bandit algorithms fail to generalize. In multi-agent systems, **different actions compete with different best responses**. This forbids us from comparing actions by comparing their own rewards, sabotaging all test-based algorithms. We solve this problem by using an Explore-then-Commit-then-Test algorithm, in combination with prior techniques. This black-box algorithm is simple but admits a no-regret guarantee.
 

 Offline Meta Reinforcement Learning with In-Distribution Online Adaptation Jianhao Wang*, Jin Zhang*, Haozhe Jiang, Junyu Zhang, Liwei Wang, Chongjie Zhang Reinforcement Learning Meta Learning ICML 2023 Why is it hard to do offline meta-learning? We point out that the meta-learning setting induces a new type of **distribution shift between offline training and online adaptation**. We characterize this challenge and propose IDAQ, a simple greedy context-based algorithm to tackle this problem.
 

 Practically Solving LPN in High Noise Regimes Faster Using Neural Networks Haozhe Jiang*, Kaiyue Wen*, Yilei Chen Theory Cryptography Deep Learning Can we break the Learning Parity with Noise (LPN) problem with Neural Networks? Empirically, we find out that **when the noise is high, neural networks are particularly useful**. We corroborate this observation by proving that the sample complexity of some neural networks scales optimally with the noise. This is the first neural-network-based algorithm surpassing all classical counterparts in breaking cryptographic primitives.
 

 Offline congestion games: How feedback type affects data coverage requirement Haozhe Jiang*, Qiwen Cui*, Zhihan Xiong, Maryam Fazel, Simon S Du Theory Game Theory ICLR 2023 If we want to learn Nash Equilibrium in congestion games, what offline dataset should we have? By looking at **dataset coverage in facility space** instead of action space, we find out the minimal dataset coverage requirement. We also present efficient learning algorithms and demonstrate the **separation** of coverage requirements under different feedback models.
 

 Offline reinforcement learning with reverse model-based imagination Jianhao Wang*, Wenzhe Li*, Haozhe Jiang, Guangxiang Zhu, Siyuan Li, Chongjie Zhang Reinforcement Learning NeurIPS 2021 You canâ€™t connect the dots looking forward; you can only connect them looking backwards. How to learn RL policy generalizing from offline datasets while avoiding dangerous actions? We propose to **learn a reverse dynamic model** instead of a forward one. If a forward model goes wrong, the learned policy may take dangerous actions. If a reverse model goes wrong, the agent still acts safely because it could not start at the wrong reversed state in the first place.
 

 &lt;/div&gt;</description></item></channel></rss>