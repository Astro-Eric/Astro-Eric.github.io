<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Paper on Haozhe Jiang</title><link>https://astro-eric.github.io/tags/paper/</link><description>Recent content in Paper on Haozhe Jiang</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 16 Nov 2025 23:29:21 +0530</lastBuildDate><atom:link href="https://astro-eric.github.io/tags/paper/index.xml" rel="self" type="application/rss+xml"/><item><title>Can Transformers Do Everything, and Undo It Too?</title><link>https://astro-eric.github.io/blogs/surjective/</link><pubDate>Sun, 16 Nov 2025 23:29:21 +0530</pubDate><guid>https://astro-eric.github.io/blogs/surjective/</guid><description>&lt;h1 id="large-language-models-are-surjective-injective-invertible"&gt;Large Language Models are Surjective? Injective? Invertible?&lt;/h1&gt;
&lt;p&gt;Recently, there have been discussions on functional properties of Transformers, the basic building block of Large Language Models (LLM) and many other generative models. &lt;a href="https://arxiv.org/abs/2508.19445"&gt;My paper&lt;/a&gt; ([1]) proves that Transformers can output anything given an appropriate input (surjective). After a few months, &lt;a href="https://arxiv.org/abs/2510.15511"&gt;a followup&lt;/a&gt; ([2]) proves that LLMs always send different inputs to different outputs (injective), and hence we can invert the outputs back to inputs. With these claims in mind, wild thoughts about generative models start flowing around. Are jailbreaks fundamentally unavoidable? Are Transformers lossless compression of knowledge? Do we have no privacy when we use LLMs? Does combining the two papers imply that LLMs are bijective? In this blog, we clarify the concepts of surjectivity, injectivity and invertibility that appear in these papers and explain their true implications.&lt;/p&gt;</description></item></channel></rss>