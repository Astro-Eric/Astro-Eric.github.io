<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Haozhe Jiang</title>
    <link>https://astro-eric.github.io/blogs/</link>
    <description>Recent content in Blogs on Haozhe Jiang</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 16 Nov 2025 23:29:21 +0530</lastBuildDate>
    <atom:link href="https://astro-eric.github.io/blogs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Can Transformers Do Everything, and Undo It Too?</title>
      <link>https://astro-eric.github.io/blogs/surjective/</link>
      <pubDate>Sun, 16 Nov 2025 23:29:21 +0530</pubDate>
      <guid>https://astro-eric.github.io/blogs/surjective/</guid>
      <description>&lt;h1 id=&#34;large-language-models-are-surjective-injective-invertible&#34;&gt;Large Language Models are Surjective? Injective? Invertible?&lt;/h1&gt;&#xA;&lt;p&gt;Recently, there have been discussions on functional properties of Transformers, the basic building block of Large Language Models (LLM) and many other generative models. A paper ([1]) proves that Transformers can output anything given an appropriate input (surjective). After a few month, a followup ([2]) proves that LLMs always send different inputs to different outputs (injective), and hence we can invert the outputs back to inputs. These claims being said, wild thoughts about generative models start flowing around. Are jailbreaks fundamentally unavoidable? Are Transfomers lossless compression of knowledge? Do we have no privacy when we use LLMs? Does combining the two papers imply that LLMs are bijective? In this blog, we clarify the concepts of surjectivity, injectivity and invertibility that appear in these papers and explain their true implications.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
